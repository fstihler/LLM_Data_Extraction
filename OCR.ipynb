{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871e1a56",
   "metadata": {},
   "source": [
    "#### Directory Structure\n",
    "In order to ensure the notebook works as intended, please match the following directory structure:\n",
    "\n",
    "\n",
    "    -- data/\n",
    "        -- pdfs/\n",
    "            -- Folder containing the folders of TCGA pathology reports downloads\n",
    "      \n",
    "        -- raw_ocr_txt/\n",
    "            -- Folder containing OCR converted pdfs as .txt \n",
    "    \n",
    "        -- clean_ocr_txt/\n",
    "            -- Folder containing .txt files after LLM pass to correct mislabelled characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6942b9",
   "metadata": {},
   "source": [
    "#### Gather PDF filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import os\n",
    "import time\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "# pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract' # For Matthew\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/Cellar/tesseract/5.3.4/bin/tesseract'\n",
    "\n",
    "file = open('./data_filepath.txt','r')\n",
    "base_dir = file.read()\n",
    "print(base_dir)\n",
    "file.close()\n",
    "pdfs_base_dir = base_dir + 'pdfs/'\n",
    "\n",
    "pdf_paths = {}\n",
    "for root, dirs, files in os.walk(pdfs_base_dir):\n",
    "    for name in files:\n",
    "        if name.endswith('.PDF'):\n",
    "            \n",
    "            uuid = name.rstrip(\".PDF\").split(\".\")[-1]\n",
    "            pdf_path = os.path.join(root, name)\n",
    "            \n",
    "            if uuid in pdf_paths.keys():\n",
    "                raise Exception(\"UUID already present in pdfs, are there duplicates?\")\n",
    "                \n",
    "            pdf_paths[uuid] = pdf_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b12297",
   "metadata": {},
   "source": [
    "#### Convert scanned pdfs to txt using OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a102c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_ocr_base_dir = base_dir + 'raw_ocr_txt/'\n",
    "\n",
    "already_converted_docs = []\n",
    "for root, dirs, files in os.walk(raw_ocr_base_dir):\n",
    "    for name in files:\n",
    "        if name.endswith('.txt'):\n",
    "            uuid = name.rstrip(\".txt\")\n",
    "            already_converted_docs.append(uuid)\n",
    "            \n",
    "\n",
    "docs = {}\n",
    "for uuid, pdf_path in tqdm(pdf_paths.items()):\n",
    "    txt_file_path = raw_ocr_base_dir + uuid + '.txt'\n",
    "    t0 = time.time()\n",
    "    print(\"Processing UUID\", uuid)\n",
    "    \n",
    "    if uuid in already_converted_docs:\n",
    "        print(\"UUID already converted - reading in txt file\\n\")\n",
    "        with open(txt_file_path, 'r') as file:\n",
    "            doc = file.read()\n",
    "        \n",
    "        docs[uuid] = doc\n",
    "        \n",
    "    else:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        pages = [pytesseract.image_to_string(image) for image in images]\n",
    "        doc = ''.join(pages)\n",
    "        \n",
    "        with open(txt_file_path, 'w') as file:\n",
    "            file.write(doc)\n",
    "            \n",
    "        docs[uuid] = doc\n",
    "        print(f\"Doc contained {len(pages)} page{'s' if len(pages) > 1 else ''}, completed in {round(time.time() - t0,2)} seconds\\n\")\n",
    "\n",
    "print(\"\\n----------------------------------\")\n",
    "print(\"       PROCESSING COMPLETE\")\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf86cc",
   "metadata": {},
   "source": [
    "#### Pass OCR text to Llama2 LLM to fix spelling/incorrect characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "api_key_path = f\"{base_dir}../assets/secrets.json\"\n",
    "with open(api_key_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    secrets = json.loads(content)\n",
    "\n",
    "\n",
    "openai = OpenAI(\n",
    "    api_key=secrets['api_key'],\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n",
    "model = \"meta-llama/Llama-2-13b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e732736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_split(doc, chunk_size=1024):\n",
    "    words = doc.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = words[i:(i+chunk_size)]\n",
    "        chunk = ' '.join(chunk)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121131f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_ocr_base_dir = base_dir + 'clean_ocr_txt/'\n",
    "\n",
    "already_cleaned_docs = []\n",
    "for root, dirs, files in os.walk(clean_ocr_base_dir):\n",
    "    for name in files:\n",
    "        if name.endswith('.txt'):\n",
    "            uuid = name.rstrip(\".txt\")\n",
    "            already_cleaned_docs.append(uuid)\n",
    "            \n",
    "\n",
    "# Define prompt for each LLM pass\n",
    "prompt = \"Please spellcheck and correct the following text (a pathology report converted to text from pdf using OCR):\\n\"\n",
    "\n",
    "# Set params for api response stream\n",
    "prompt_tokens = 2048\n",
    "max_tokens = 4096 - prompt_tokens\n",
    "stream = True\n",
    "\n",
    "# Text correction and recovery\n",
    "cleaned_docs = {}\n",
    "chunk_size = 512\n",
    "total_tokens = 0\n",
    "total_words = 0\n",
    "for uuid in tqdm(docs):\n",
    "        txt_file_path = clean_ocr_base_dir + uuid + '.txt'\n",
    "        t0 = time.time()\n",
    "        print(\"\\nProcessing UUID\", uuid)\n",
    "\n",
    "        if uuid in already_cleaned_docs:\n",
    "            print(\"UUID already cleaned - reading in txt file\\n\")\n",
    "            with open(txt_file_path, 'r') as file:\n",
    "                doc = file.read()\n",
    "\n",
    "            cleaned_docs[uuid] = doc\n",
    "\n",
    "        else:\n",
    "            raw_doc = '\\n\\n[Begin Document]\\n' + docs[uuid] + '[End Document]'\n",
    "            total_words += len(raw_doc.split())\n",
    "            raw_chunks = chunk_split(raw_doc, chunk_size)\n",
    "            \n",
    "            clean_chunks = []\n",
    "            for raw_chunk in raw_chunks:\n",
    "                cat_prompt = \" \".join([prompt, raw_chunk])\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": cat_prompt}],\n",
    "                    stream = stream, \n",
    "                    max_tokens=max_tokens \n",
    "                )\n",
    "            \n",
    "            \n",
    "\n",
    "                for event in response:\n",
    "                    if hasattr(event, 'error_type'):\n",
    "                        print(event)\n",
    "                        print(\"_______________________________\")\n",
    "                        print(\"This document failed to process\")\n",
    "                        token_count = 'unknown'\n",
    "                    else:\n",
    "                        tkn = event.choices[0].delta.content\n",
    "                        if tkn is not None:\n",
    "                            clean_chunks.append(tkn)\n",
    "                        else:\n",
    "                            token_count = event.usage['total_tokens']\n",
    "                            total_tokens += token_count\n",
    "                            \n",
    "            doc = ''.join(clean_chunks)\n",
    "            \n",
    "            with open(txt_file_path, 'w') as file:\n",
    "                file.write(doc)\n",
    "    \n",
    "            cleaned_docs[uuid] = doc\n",
    "            if not token_count == 'unknown':\n",
    "                print(f\"Cleaning completed in {round(time.time() - t0,2)} seconds, cost {token_count} tokens\")\n",
    "\n",
    "                \n",
    "print(f\"\\n\\nFinal Cost: {total_tokens} tokens\")\n",
    "print(f\"Average token / word: {total_tokens / total_words}\")\n",
    "print(\"\\n----------------------------------\")\n",
    "print(\"       PROCESSING COMPLETE\")\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb3404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
